\section{Technische Analyse von RSTensorFlow}
\label{sec:analyserstf}
Bei TensorFlow handelt es sich um ein umfangreiches und komplexes Projekt, weshalb eigene Anpassungen ein gutes Verständnis dieses Deep Learning Framworks erfordern. In diesem Kapitel wird zunächst eine technische Analyse des Quellcodes im Hinblick auf die von RSTensorFlow vorgenommenen Anpassungen in \ref{subsec:quellcodeanalyse} durchgeführt. Hier wird die Quellcode-Struktur von TensorFlow im Allgemeinen, sowie die Anpassungen von RSTensorFlow erläutert. Der Build-Prozess für die Android-Demo wird in \ref{subsec:buildprozess} untersucht. 
\\
Die von TensorFlow bereits implementierten Demos für Android sind in Java geschrieben und verwenden das Android SDK. Das Deep Learning Framework TensorFlow hingegen ist mit C++ umgesetzt. Als Schnittstelle zwischen TensorFlow und Android wird das Android Native Development Kit (NDK) eingesetzt. Das NDK ermöglicht die Verwendung von C/C++ auf einer Android-Plattform und nutzt die Java Native Interface (JNI) API. \\
Das Framework TensorFlow wird für die Android-Demos in die Shared Object Datei \textit{libtensorflow\_inference.so} gepackt. In dieser .so-Datei befindet sich ebenfalls das \textit{TensorFlowInferenceInterface}, welches den Zugriff der Android-App auf TensorFlow durch das NDK realisiert. Somit können Deep Learning Anwendungen für Android auf diesen Kern aufsetzen und bequem Android-Activities, welche auf das Inference Interface zugreifen, umgesetzt werden. 

\subsection{Analyse des Quellcodes}
\label{subsec:quellcodeanalyse}
In TensorFlow ist für die Android-Demo Apps ein eigenes Teilprojekt definiert. Dieses beinhaltet die Activity-Klassen der in \ref{subsec:androiddemo}  beschriebenen drei Apps. Die TF Classify App wird auf Basis der \textit{ClassifierActivity.java} erstellt. Hier ist auch der Pfad zur verwendeten Model-Datei innerhalb der späteren APK deklariert. Diese enthält das von TF Classify verwendete Google Inception Model zur Klassifizierung von Objekten und liegt als Protocol Buffer (.pb) vor. Mit TensorFlow und TensorBoard kann dieses bei Bedarf visualisiert werden, um so ein tieferes Verständnis bei der Objektklassifizierung von TF Classify und der durchgeführten Operationen zu erlangen. 
\\
Für die meisten Deep Learning Operationen wird von TensorFlow die in C++ geschriebene Template-Bibliothek \textit{Eigen} \cite{eigenlibrary} verwendet. Diese berechnet Operationen der linearen Algebra schnell und achtet dabei auf den verbrauchten Cache-Speicher. Eigen wird somit auch für die Matrixmultiplikation und die Convolution-Operation verwendet. Damit eine substituierende Verwendung von RenderScript möglich ist, müssen die Aufrufe der Eigen-Bibliothek im Kernel von TensorFlow ersetzt werden. RenderScript ist jedoch für die direkte Verwendung in Java konzipiert, weshalb die Integration in den TensorFlow-Kernel komplex ist. Die Autoren von \cite{rstensorflow2017} haben zur Lösung des Problems für den Quellcode von RenderScript einen Wrapper in C++ umgesetzt, welcher das Skript aufruft. Die RenderScript-Dateien werden in separaten Shared Object Dateien gekapselt und stehen somit der Anwendung zur Verfügung. Für RSTensorFlow wurden so die Operationen matmul und conv2D in RenderScript umgesetzt. Damit TensorFlow statt der Eigen-Implementierung die RS-Variante nutzt, sind Anpassungen am Kernel von TensorFlow nötig. Hier müssen die Quelldateien 
\begin{itemize}
	\item conv\_ops.cc und
	\item matmul\_op.cc
\end{itemize}
im Kernel so geändert werden, dass statt des Aufrufs der Eigen-Bibliothek die jeweilige Funktion des RenderScript-Wrappers aufgerufen wird. Da es sich um eine Änderung am Kernel handelt, wirken sich diese direkt auf alle darauf aufsetzenden Funktionen aus. An den einzelnen Android-Activities ist daher keinerlei weitere Anpassung nötig. Die TF Classify App nutzt somit automatisch die RenderScript-Implementierung. 

\subsection{Der Build-Prozess}
\label{subsec:buildprozess}
Für den Build-Prozess von TensorFlow wird das von Google entwickelte Opensource Tool Bazel (\url{https://bazel.build/}) verwendet. Bazel automatisiert den Build und unterstützt standardmäßig mehrere Programmiersprachen wie Java und C++. Durch eine \textit{WORKSPACE}-Datei wird ein Ordner und dessen Inhalt als Bazel Workspace definiert. Ebenso werden in der WORKSPACE-Datei externe Abhängigkeiten des Projekts, wie beispielsweise das Android SDK, referenziert. \textit{BUILD}-Dateien innerhalb des Workspaces definieren Packages, welche der Organisation des Projekts dienen. Innerhalb der BUILD-Datei können Labels, Build-Regeln und mehr definiert werden. 
\\
Das Projekt für die in \ref{subsec:quellcodeanalyse} beschriebenen Shared Objects der RenderScript-Implementierung ist innerhalb der Projektstruktur von TensorFlow im Contribution-Ordner als neues Package angelegt. Im Contrib-Ordner werden Teilprojekte gesammelt, die irgendwann ein fester Bestandteil des Kerns von TensorFlow werden können. 
\\
Um die Android-Demo von TensorFlow auf Basis des Quellcodes zu bauen, wird \textit{bazel build} mit dem Build-Target \textit{//tensorflow/examples/android:tensorflow\_demo} aufgerufen. Getrennt durch einen Doppelpunkt gibt der vordere Teil des Textes den Pfad der BUILD-Datei relativ zum Workspace an. Der hintere Teil spezifiziert die für den Build anzuwendende Regel. Bei \textit{tensorflow\_demo} wird eine Android-Regel angesprochen, welche eine APK als Ergebnis hervorbringt. Hier wird unter anderem das TensorFlow Inference Interface als Abhängigkeit eingebunden. Ebenso wird TensorFlow selbst als C/C++-Regel in den Prozess eingebunden. Für die Integration des RenderScript-Projekts im Contribution-Ordner wird dieses Package innerhalb der C/C++-Regel für die TensorFlow-Bibliotheken hinzugefügt. Jedoch wird hier lediglich der Ordner mit den bereits vorliegenden .so-Dateien eingebunden. Allerdings beinhaltet dieser Ordner nicht die bereits kompilierte \textit{librs.mScriptConv.so}, welche die in RenderScript implementierten Operationen matmul und conv2D enthält. Um den Build-Prozess nicht zu verändern, wird das Projekt manuell neu kompiliert und die resultierende \textit{librs.mScriptConv.so} in den entsprechenden Ordner kopiert. 
